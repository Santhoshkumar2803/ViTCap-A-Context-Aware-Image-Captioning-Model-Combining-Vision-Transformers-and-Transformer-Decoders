{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11741227,"sourceType":"datasetVersion","datasetId":7370629},{"sourceId":11741283,"sourceType":"datasetVersion","datasetId":7370655},{"sourceId":238536010,"sourceType":"kernelVersion"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nnltk.download('punkt_tab')\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:37:47.283118Z","iopub.execute_input":"2025-05-10T04:37:47.283352Z","iopub.status.idle":"2025-05-10T04:37:49.333495Z","shell.execute_reply.started":"2025-05-10T04:37:47.283326Z","shell.execute_reply":"2025-05-10T04:37:49.332668Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.models import vit_b_16\nfrom PIL import Image\nimport os\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\nimport numpy as np\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nclass Config:\n    def __init__(self):\n        self.image_dir = '/kaggle/input/newdata1/Flicker8k_Dataset'\n        self.captions_file = '/kaggle/input/newdata/Flicker8k-captions/Flickr8k.token.txt'\n\n        self.checkpoint_dir = '/kaggle/working/checkpoints'\n\n\n        self.embed_size = 256\n        self.hidden_size = 512\n        self.num_layers = 6\n        self.num_heads = 8\n        self.num_epochs = 10\n        self.batch_size = 32\n        self.lr = 0.001\n        self.max_seq_length = 30\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n        if not os.path.exists(self.image_dir):\n            raise FileNotFoundError(f\"Image directory not found: {self.image_dir}\")\n        if not os.path.exists(self.captions_file):\n            raise FileNotFoundError(f\"Captions file not found: {self.captions_file}\")\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n\nclass ImageCaptionDataset(Dataset):\n    def __init__(self, image_dir, captions_file, vocab, transform=None):\n        self.image_dir = image_dir\n        self.captions_file = captions_file\n        self.vocab = vocab\n        self.transform = transform\n        self.imgs, self.captions = self.load_data()\n\n    def load_data(self):\n        imgs = []\n        captions = []\n        with open(self.captions_file, 'r') as file:\n            lines = file.readlines()\n            for line in lines:\n                img_name, caption = line.strip().split('\\t')\n                img_name = img_name.split('#')[0]\n                imgs.append(img_name)\n                caption_tokens = word_tokenize(caption.lower())\n                captions.append(\n                    [self.vocab['<start>']] +\n                    [self.vocab[word] for word in caption_tokens if word in self.vocab] +\n                    [self.vocab['<end>']]\n                )\n        return imgs, captions\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        img_name = self.imgs[idx]\n        caption = self.captions[idx]\n        img_path = os.path.join(self.image_dir, img_name)\n\n        try:\n            img = Image.open(img_path).convert('RGB')\n            if self.transform:\n                img = self.transform(img)\n            return img, torch.tensor(caption)\n        except FileNotFoundError:\n            new_idx = np.random.randint(0, len(self))\n            return self.__getitem__(new_idx)\n\n\ndef collate_fn(batch):\n    imgs, captions = zip(*batch)\n    imgs = torch.stack(imgs, 0)\n    captions = pad_sequence(captions, batch_first=True, padding_value=2)\n    return imgs, captions\n\n\n\n    def __len__(self):\n        return len(self.imgs)\n\n    def __getitem__(self, idx):\n        img_name = self.imgs[idx]\n        caption = self.captions[idx]\n\n        img_path = os.path.join(self.image_dir, img_name)\n\n        try:\n            img = Image.open(img_path).convert('RGB')\n            if self.transform:\n                img = self.transform(img)\n            return img, torch.tensor(caption)\n\n        except FileNotFoundError:\n\n            new_idx = np.random.randint(0, len(self))\n            return self.__getitem__(new_idx)\n\n    from torch.nn.utils.rnn import pad_sequence\n\n\n\ndef collate_fn(batch):\n    imgs, captions = zip(*batch)\n    imgs = torch.stack(imgs, 0)\n\n    captions = pad_sequence(captions, batch_first=True, padding_value=2)\n\n    return imgs, captions\n\n\n\n\nclass EncoderViT(nn.Module):\n    def __init__(self, embed_size):\n        super(EncoderViT, self).__init__()\n\n        self.vit = vit_b_16(weights='DEFAULT')\n\n\n        self.vit.heads = nn.Linear(self.vit.hidden_dim, embed_size)\n\n\n        self.embed_size = embed_size\n\n    def forward(self, images):\n\n        features = self.vit(images)\n        return features\n\n\nclass DecoderTransformer(nn.Module):\n    def __init__(self, embed_size, hidden_size, num_layers, num_heads, vocab_size):\n        super(DecoderTransformer, self).__init__()\n        self.embed_size = embed_size\n\n\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n\n\n        decoder_layer = nn.TransformerDecoderLayer(\n            d_model=embed_size,\n            nhead=num_heads,\n            dim_feedforward=hidden_size,\n            dropout=0.1\n        )\n\n\n        self.transformer_decoder = nn.TransformerDecoder(\n            decoder_layer=decoder_layer,\n            num_layers=num_layers\n        )\n\n\n        self.fc_out = nn.Linear(embed_size, vocab_size)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, features, captions):\n\n        embeddings = self.embedding(captions)\n\n\n        tgt_mask = self.generate_square_subsequent_mask(captions.size(1)).to(features.device)\n\n\n        embeddings = embeddings.permute(1, 0, 2)\n\n\n        memory = features.unsqueeze(0).repeat(embeddings.size(0), 1, 1)\n\n\n        output = self.transformer_decoder(\n            tgt=embeddings,\n            memory=memory,\n            tgt_mask=tgt_mask\n        )\n\n\n        output = output.permute(1, 0, 2)\n\n\n        output = self.fc_out(output)\n        return output\n\n    def generate_square_subsequent_mask(self, size):\n        mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n        return mask\n\n\ndef build_vocab(captions_file, threshold):\n    vocab = {'<start>': 0, '<end>': 1, '<pad>': 2}\n    word_count = Counter()\n\n    with open(captions_file, 'r') as file:\n        lines = file.readlines()\n        for line in lines:\n            caption = line.strip().split('\\t')[1]\n            tokens = word_tokenize(caption.lower())\n            word_count.update(tokens)\n\n    idx = 3\n    for word, count in word_count.items():\n        if count >= threshold:\n            vocab[word] = idx\n            idx += 1\n\n    return vocab\n\n\ndef train():\n    config = Config()\n\n\n    vocab = build_vocab(config.captions_file, threshold=5)\n    vocab_size = len(vocab)\n\n\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n\n    dataset = ImageCaptionDataset(config.image_dir, config.captions_file, vocab, transform)\n    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True, collate_fn=collate_fn)\n\n\n\n    encoder = EncoderViT(config.embed_size).to(config.device)\n    decoder = DecoderTransformer(config.embed_size, config.hidden_size, config.num_layers, config.num_heads, vocab_size).to(config.device)\n\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=config.lr)\n\n\n    for epoch in range(config.num_epochs):\n        for imgs, captions in dataloader:\n            imgs, captions = imgs.to(config.device), captions.to(config.device)\n\n\n            features = encoder(imgs)\n            outputs = decoder(features, captions[:, :-1])\n\n\n            loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].contiguous().view(-1))\n\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch [{epoch+1}/{config.num_epochs}], Loss: {loss.item():.4f}\")\n\n\n        if (epoch+1) % 5 == 0:\n            torch.save({'epoch': epoch+1, 'encoder': encoder.state_dict(), 'decoder': decoder.state_dict(), 'optimizer': optimizer.state_dict()},\n                       os.path.join(config.checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pth\"))\n\nif __name__ == '__main__':\n    train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T04:37:52.870246Z","iopub.execute_input":"2025-05-10T04:37:52.870513Z","iopub.status.idle":"2025-05-10T06:52:01.336954Z","shell.execute_reply.started":"2025-05-10T04:37:52.870493Z","shell.execute_reply":"2025-05-10T06:52:01.336066Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n100%|██████████| 330M/330M [00:04<00:00, 82.3MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 3.6211\nEpoch [2/10], Loss: 3.0636\nEpoch [3/10], Loss: 3.9261\nEpoch [4/10], Loss: 3.8701\nEpoch [5/10], Loss: 3.8345\nEpoch [6/10], Loss: 3.7778\nEpoch [7/10], Loss: 2.9669\nEpoch [8/10], Loss: 3.5022\nEpoch [9/10], Loss: 2.9013\nEpoch [10/10], Loss: 3.5171\n","output_type":"stream"}],"execution_count":2}]}